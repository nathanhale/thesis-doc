

\chapter{Implementation}

This chapter describes how the method of Chapter 3 was actually implemented for this project, a non-trivial step considering the major difficulties associated with the vast amount of data used. This includes discussion of the way in which some of the parameters of the methodology were instantiated in addition to the specific implementation details for this project. It begins by describing how the dataset was selected and the challenges that the large dataset presented in terms of storing and accessing the data. From there it moves on to describe how the bipartite graph was constructed, including selection of tweets and users, construction of edges,  and initialization of scores. From there, the actual implementation of the algorithm is described both in words and in pseudocode before finally describing the method of retrieving the results from the database.

Java was chosen as the programming language for this project due to the wide adoption of libraries available to interface with it and the ease of development with those libraries. The Lucene information retrieval library\footnote{http://lucene.apache.org/core/}, for example, is originally written in Java and was used to index the tweets in a way that made tf-idf cosine-similarity calculations easy. It also facilitated easy search of the tweet database which was useful during the development process to learn more about the nature of the dataset. In addition to Lucene, the Java Database Connectivity library (JDBC) was used to interact with the PostgreSQL database and the Stanford named-entity recognizer was used for the named entity recognition.

\section{Selecting a Dataset}
\label{sec:SelectingADataset}

When beginning the project, one of the first questions was which dataset to use. There were four possibilities: downloading data via the Twitter API specifically for this project, the Choudhury data set (\cite{Choudhury2010}), the Stanford data set (\cite{Yang2011}) using the social graph of Kwak et. al. (\cite{Kwak2010}), and the database created for the Text Retrieval Conference (TREC). Each had advantages and disadvantages.

Given the availability of existing datasets that would meet the needs of the project, the possibility of downloading a social graph and set of tweets specifically for this project was dismissed early on, leaving the task as one of selecting which of the three datasets was best.

The smallest dataset was the Choudhury data. It consisted of tweets collected between late 2008 and late 2009 and contained more than 10 million tweets and more than 800,000 edges. This database is much smaller than the network was at the time it was created; it contains a tiny fraction of the tweets which were produced during that time and a much smaller fraction of the number of users. It was created by starting a seed set of users and then building the database out from there, but does not include the complete set of followers or followees for any user, though it is a connected graph. For example, a number of users have a list of some followers and then are shown to follow no one. This lack of completeness made this dataset unacceptable for this project.

The second dataset was the TREC dataset\footnote{http://trec.nist.gov/data/tweets/}, consisting of 16 million tweets collected over a two week period in late January and early February of 2011. It is not intended to be a complete list of the tweets from that time period, merely a representative sample which includes spam tweets amongst the important tweets. The major problem with this dataset, however, is that it did not include any social graph information, just the tweets. Since a major portion of this project was based on the impact of the social graph on the recommendations, this clearly was not possible to use.

The Stanford Twitter dataset contains 476 million tweets from more than 17 million users. These tweets were collected over a period of 7 months between 1 June, 2009 and 31 December, 2009. The authors of \cite{Yang2011} estimate that these tweets covered approximately 20-30\% of the tweets published in that time period\footnote{http://snap.stanford.edu/data/twitter7.html}. By itself, this dataset would be unusable for the same reason as the TREC database since it does not include any social graph information. Fortunately the authors link to the published social graph of \cite{Kwak2010}, which was crawled via the Twitter API contemporaneously with the collection of the Stanford tweet data. Though the vastness of this dataset presented many challenges during the project, it also was the best choice for putting together a recommendation system to be as accurate as possible.

Of these three datasets, only the TREC tweet data and the social graph of \cite{Kwak2010} are currently available to the public. Twitter changed their terms of service, requiring care to be taken with the data and researchers to remove tweets from their databases if they were deleted by their original authors. The TREC data is accompanied by a tool which queries the Twitter service to accomplish this, which is why it remains, though even it requires that the United States National Institute of Standards and Technology issue a password to allow the data to be downloaded.

The tweets from the Stanford dataset were able to be used here because one of the researchers within the Oxford University Department of Computer Science had downloaded it before this change came into effect.


\section{Data Storage}
\label{sec:DataStorage}

Considering the vast amount of data available in the dataset, an efficient data storage mechanism was vital. The major consideration was the ease and efficiency of retrieving the data. PostgreSQL was selected to store the information because it has excellent support and is a mature database management system capable of being used with large amounts of data. It has a good interface with the Java programming language and is also open source, making it possible to tweak the implementations of certain features if necessary, though that was not done here.

The biggest source of data to be put into the PostgreSQL database was the social graph. The tweets took up far more disk space, but not all of them needed to be stored in the database because most of them were not used, as described in Section~\ref{sec:BuildingTheGraph}. The complete social graph, meanwhile, was approximately 24 GB, with each line containing a pair of user ids separated by a tab delimiter. Unfortunately the tweets data from Stanford had each tweet indicated only by the user's name, meaning that the data from the social graph which had only user id pairs had to be translated before being stored.

The final \texttt{namenetwork} table, whose schema is shown in Table~\ref{tab:namenetworkSchema}, used the actual usernames as looked up from another database that accompanied the social graph data. Indices were created around both the username and followername columns, with the table being clustered on the username index since it is used the most frequently and has the most data returned from it since the most popular users have far more followers than followees.

Still, it was helpful for certain queries to have fast access to the followees of a given user, so a second copy of the namenetwork table was created to make these queries faster---this version was called \texttt{namenetwork\_followers} and had the same schema as the namenetwork table but was clustered on the followername index rather than the username index.

The usernames were all converted to lower case, and indeed all usernames which could potentially be parsed in mixed case found while indexing the tweets were converted to lower case. Additionally, the social graph data contained a number of duplicate user names. This happened because the program which aggregated the data apparently ran over an extended period of time over which some user accounts were deleted and re-created. Additionally, a large number of user ids corresponded to the name `n/a'. All of these duplicate names were removed from the database.

\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
username & VARCHAR & The name of a user \\ \hline
followername & VARCHAR & The name of a user who follows the user in the username column for this row \\
\end{tabular}
\caption{Schema of the `namenetwork' table}
\label{tab:namenetworkSchema}
\end{table}


For the initialization of the user scores (cf. Section~\ref{sec:InitializingScores}) using the method of Adamic and Adair (cf. Section~\ref{sec:InitializationOfUserScores}) it was necessary to know how many followers each user had. This could be calculated on-the-fly by running a simple SQL \texttt{COUNT()} query on the namenetwork table, but this would sacrifice performance, so it made more sense to do this query once and create a table for it. The \texttt{followercount} table, whose schema is shown in Table~\ref{tab:followercountSchema}, was created using the following SQL command:

\begin{verbatim}
  CREATE TABLE followercount AS
      (SELECT username, COUNT(followername) AS count
          FROM namenetwork GROUP BY username);
\end{verbatim}

\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
username & VARCHAR & The name of a user \\ \hline
count & INT & The number of followers of the user in the username column for this row \\
\end{tabular}
\caption{Schema of the `followercount' table}
\label{tab:followercountSchema}
\end{table}

\noindent
A hash index was created on the username column of the followercount table to allow for quickly looking up the number of followers that a user has.


The tweets themselves were not all used in any given run of the algorithm, and in fact only a tiny portion of them were used. One guideline for the project was that the recommendations themselves be only based on tweets from a particular time period on the order of one to two weeks in order to reflect fleeting interests and recommend things that are of interest to the user at that time. For example, during the Olympics someone may be very interested in tweets about the Olympics, but they would probably not be so interested a few weeks after the Olympics have ended. For this project 500,000 tweets were indexed when the algorithm was initialized, corresponding to about 15 hours of the data stretching between 21:03 on 30 November, 2009 to 11:15 on 1 December, 2009. Considering that the full dataset contains nearly half a billion tweets, this is a very small portion.

It was useful to store this small portion of tweets in two different ways for two different purposes. For purposes of initializing the tweet scores (again, cf. Section~\ref{sec:InitializingScores}) it was very useful to index the tweets in a Lucene index because such indices automatically count the frequency of each term in each document and in the document collection as a whole, making tf-idf cosine similarity scoring easy.

At the same time, it was also valuable to be able to write SQL statements to quickly select particular tweets and to update their Co-HITS scores quickly, a task much better accomplished via a relational database such as PostgreSQL. This storage of the tweets represented the tweet vertices in the graph, and thus information was included about the Co-HITS score and the original Co-HITS score for each of these nodes. The schema for the \texttt{tweet\_vertices} table is shown in Table~\ref{tab:tweetverticesSchema}. This table was indexed on the tweetid field using a btree index. After the initial index was built, an operation which only needed to happen one time for the series of tweets under investigation, the index was clustered. During the creation of the edges this table was clustered on the names index since that was how most of the edges were created---by joining user vertices with their tweets. Once this was completed, the tweets were clustered based on their tweetids for actually running the algorithm.

\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
tweetid & INT & The unique id of the tweet that this vertex represents \\ \hline
name & VARCHAR & The author of the tweet that this vertex represents \\ \hline
date & TIMESTAMP  & The timestamp of when the tweet that this vertex represents was published \\ \hline
tweet & VARCHAR & The text of the tweet that this vertex represents \\ \hline
score & FLOAT & The Co-HITS score for this tweet \\ \hline
original\_score & FLOAT & The original Co-HITS score for this tweet before the algorithm's first iteration. \\
\end{tabular}
\caption{Schema of the `tweet\_vertices' table}
\label{tab:tweetverticesSchema}
\end{table}

The tweets were stored in the database, so it obviously makes sense that the user vertices would be stored in the database as well. The schema for the \texttt{user\_vertices} table is shown in Table~\ref{tab:userverticesSchema}. Similar information needed to be tracked with these vertices as with the tweet vertices, most notably the current Co-HITS score and the original Co-HITS score. The index on this table was a btree index on the name field, and as with the tweet vertices this index was clustered after it was populated since this only needed to happen one time.

\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
name & VARCHAR & The name of the user that this vertex represents \\ \hline
score & FLOAT & The Co-HITS score for this user \\ \hline
original\_score & FLOAT & The original Co-HITS score for this user before the algorithm's first iteration. \\
\end{tabular}
\caption{Schema of the `user\_vertices' table}
\label{tab:userverticesSchema}
\end{table}

Storage in a PostgreSQL database facilitated some easy implementations of certain operations. For example, by storing the original score in the vertex it was possible to quickly determine whether the original scores had been initialized and normalized by simply checking whether the sum of the original\_score column was equal to one:

\begin{verbatim}
SELECT SUM(original_score) FROM user_vertices;
\end{verbatim}

\noindent
Similarly, the scores could be normalized with only one SQL command:

\begin{verbatim}
UPDATE tweet_vertices
    SET original_score=(original_score / S.sum)
    FROM (SELECT SUM(original_score) FROM tweet_vertices) S;
\end{verbatim}

\noindent
And if the algorithm needed to be reset so that it could be run again, it was a simple matter of copying over the original\_score field to the score field. In the actual implementation, three additional columns were present in the two vertex tables, one for each of the distinguished users. These columns stored the original scores for each of the users and made it possible to calculate this value only once for each user, facilitating faster testing.

The final component that was stored in the PostgreSQL database was the list of edges between the two different classes of vertices. The schema of the \texttt{edges} table is shown in Table~\ref{tab:edgesSchema} and is quite basic. The name of the user and the id of the tweet that are connected is shown, along with the type of the edge. The directionality (if any) of that edge is indicated purely by its type. Experimentation with making edges of particular types directed in particular ways is addressed in Section~\ref{sec:VaryingEdgeDirectionality}.


\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
name & VARCHAR & The name of the user that this edge connects to. \\ \hline
tweetid & INTEGER & The unique identifier of the tweet that this edge connects to. \\ \hline
type & INTEGER & The type of edge that this represents (see Table~\ref{tab:EdgeTypes}). \\
\end{tabular}
\caption{Schema of the `edges' table}
\label{tab:edgesSchema}
\end{table}

As with the \texttt{namenetwork} table, different clusterings were useful for different purposes. For purposes of running the actual algorithm, it was necessary to sometimes order the edges by username and sometimes by tweetid. The edges table was clustered around the index of the username column, so when retrieving the edges in order of tweetid, the query took far more time: two orders of magnitude more, in fact. Thus a copy of the edges table called \texttt{edges\_tweetids} was created for this purpose, this one clustered around the index of the tweetids, allowing both queries to be much faster.

\section{Building the Graph}
\label{sec:BuildingTheGraph}

Building the graph is a very important part of the process. First the users and tweets are selected and indexed and then all the appropriate edges between the two sides of the bipartite graph are created.

\subsection{Selecting Users and Tweets}

How to build the graph itself is not as simple a question as it would seem to be at first. Which users should be present in the graph? Which tweets should be present? How the graph is created and the decisions about which nodes are present dictates how connected the graph will be and thus how the scores from each set of vertices will interact with one another. Having users who are not connected to the rest of graph serves no purpose because no information about their suitability can be gained.

Tweets were added by opening one of the files containing tweet data and parsing until the appropriate number of tweets had been indexed. Certain tweets in the data were invalid and were thrown out. The tweet ids were created by beginning at 1 and then incrementing the id for each tweet that was added to the database (i.e. each valid tweet). Proceeding in this manner ensured that the tweets were all from the same time period, all were valid, and all had a unique identifier.

In order to ensure that the graph was maximally connected, only users who would have connections were added. Practically speaking, this meant that users were added as the tweets were placed into the tweet\_vertices database. All users who had authored a tweet were added, but so were all users who had been retweeted or mentioned. Because of the types of edges created, as described in Section~\ref{sec:EdgeCreation}, these users are guaranteed to at least be connected to one tweet and will likely be connected to several others.


\subsection{Edge Creation}
\label{sec:EdgeCreation}

Once the users and tweets have been added to the database, the next step is to add the edges between the users and the tweets. There were twelve different types of edges used in this project, though the list is by no means exhaustive of the possibilities. The different edge types are listed in Table~\ref{tab:EdgeTypes}. 

The authorship edges are simple and are actually created when the users and tweets are initially indexed. For each tweet added to the database its author is added if that user is not already present and then an edge is added between the tweet and the author. These edges are simple.

The remainder of the edge types are more complicated and fall broadly into two categories. The first category is the follower, retweet, mention, and @reply edges, which correspond to the network edges connecting users to other users and the edges connecting users to particular tweets. These edges all have an obvious directionality implied by the type of edge. The second category of edge is the content edges which correspond to the edges between tweets, and these edges don't have any obvious directionality.


\begin{table}
\centering
\begin{tabular}{p{1.9cm}|p{9cm}|p{1.9cm}}
{\bf Edge Type} & {\bf Description} & {\bf Direction} \\ \hline
Authorship & Connects a user to a tweet authored by that user & User $\rightarrow$ Tweet \\ \hline
Follower & Connects a user to a tweet authored by one of their followees, as described in Section~\ref{sec:ProjectingToBipartite} & Tweet $\rightarrow$ User \\ \hline
Retweet & Connects the tweet to the person being retweeted & User $\rightarrow$ Tweet \\ \hline
Retweet Followees & Connects the tweets of the followees of the person being retweeted to the retweeter & Tweet $\rightarrow$ User \\ \hline
Retweet Followers & Connects the tweets of the person being retweeted to the followers of the retweeter & Tweet $\rightarrow$ User \\ \hline
Mention & Connects the tweet to the person being mentioned & User $\rightarrow$ Tweet \\ \hline
Mention Followees & Connects the tweets of the followees of the person being mentioned to the mentioner & Tweet $\rightarrow$ User \\ \hline
Mention Followers & Connects the tweets of the person being mentioned to the followers of the mentioner & Tweet $\rightarrow$ User \\ \hline
At Reply & Connects the tweet to the person being @replied to & User $\rightarrow$ Tweet \\ \hline
At Reply Content & Connects the tweets of the person being @replied to to the tweeter & Tweet $\rightarrow$ User \\ \hline
Hashtag & Connects all tweets which use a given hashtag to the authors of those tweets & Bi-Directional \\ \hline
Content & Connects all tweets discussing similar content (cf. Section~\ref{sec:DeterminingContentSimilarity}) to the authors of those tweets. & Bi-Directional \\
\end{tabular}
\caption{The types of edges in the graph}
\label{tab:EdgeTypes}
\end{table}


\subsubsection{Network Edges}

The follower edges are the most straightforward to describe and are simply the result of following the procedure for projection of user connections described in Section~\ref{sec:ProjectingToBipartite}.

The retweet and mention edges are more complicated but are very analogous to one another. The simplest of these edges is the simple retweet or mention edge which connects the tweet to the person being mentioned or retweeted; as with the authorship edges, these are created at the time that the tweets are initially indexed. These edges capture the intuition that a high scoring tweet which mentions or retweets someone suggests that the person in question should also be scored highly and that a person is likely to be interested in a tweet that mentions them or comments on something they say. The followees and followers versions of the retweet and mention edges, meanwhile, are more complicated.

The followees edge creates a connection between the person who is retweeting or mentioning someone and the followees of the person being mentioned or retweeted. This is because that user has proven by their retweet or mention that they are very interested in content from the user who authored the original tweet, so it stands to reason that they are correspondingly more likely to be interested in the same things as that person.

The followers edge connects the tweets of the person who was retweeted or mentioned to all of the followers of the person who retweeted or mentioned them. If a user is interested enough in someone to follow them and see their tweets then it stands to reason that they would also be interested in someone that their followee is very interested in.

The @reply edges are quite straightforward, though they generally only reinforce the existing follower and followee connections. These edges connect the tweet to the person the @reply was directed at and connect the tweets of the person the @reply was directed towards to the person who sent it.


\subsubsection{Content Edges}

There are far fewer content edges, but they convey a great deal of information. These edges are based on the system of projecting edges between tweets into the bipartite graph by connecting the edges to users instead using the procedure detailed in Section~\ref{sec:ProjectingToBipartite}.

Hashtag edges represent the connections between tweets using the same hashtag. These edges connect all the tweets using a particular hashtag to all of the authors of tweets containing those hashtags. Figure~\ref{fig:content_projection} demonstrates this process. The hashtags are converted to lowercase for normalization purposes and because Twitter itself treats hashtags as being case insensitive.

The content edges connect tweets in the same way, but using the implicit links between tweets with similar content as the basis for the projection. How the similarity in content is determined can vary between implementations and range from something complicated such as Latent Dirichlet Allocation topic models to something much simpler.

\subsection{Determining Content Similarity}
\label{sec:DeterminingContentSimilarity}

Determining which tweets contain similar content to one another is an important part of the algorithm used here because it provides many connections between users and tweets that are not connected via straightforward network connections, allowing novel tweets to be recommended by the algorithm.

The method for determining this similarity need not be very complicated, though it certainly can be. Using topic models and other techniques from statistics and machine learning would certainly generate good results for determining content similarity, but at the cost of being quite computationally expensive.

For this project a much simpler and faster method was used: named-entity recognition (NER). Using the Stanford Named-Entity Recognizer \cite{Finkel2005}, each tweet was run through the tool in order to find any named entities such as people, places, and organizations. This was done as the tweets were being processed for other content information such as hashtags.

Each named entity found was converted to lower case and stored in memory along with the id of the tweet in which it appeared. If another tweet was found containing the same named entity then that tweet's id was stored along with the ids of the other tweets already found with that id. Once all of the tweets had been processed, each list of tweetids was connected to the authors of the respective tweets.

This process is far from perfect, but provides a good approximation of the people, places, and things that users are talking about. In experiments done prior to incorporating this tool into the algorithm, the NER of the Stanford library was found to be generally accurate, despite the lack of context resulting from the space constraints of tweets.

The biggest problem with the use of the recognizer in this algorithm is that it returns compound entities as separate entities, meaning that entities such as `United Kingdom' would be returned as two separate words. Similarly, names when given as first and last names (e.g. `David Beckham') would be returned in two chunks. This presents a question as to whether these words should be treated individually or combined into one entity.

Leaving them as separate words gives some poor results such as `David' or `United' being counted as entities even though they are not very useful, while combining them separates cases where people are referred to by last name, meaning that `Beckham' and `David Beckham' would be two different entities even though they refer to the same thing. Similarly, phrases such as `David Beckham, United Kingdom' would be returned as one big entity, which is clearly nonsensical.

For the purposes of this project, the former approach of leaving all entities as separate was taken. This leads to a small number of garbage edges but with the benefit of more correct connections and a simpler implementation. Put another way, this choice increases the recall of the connections between content while decreasing the precision.


\subsection{Graph Statistics}

The average degree of a user node and a tweet node can be taken by dividing the number of edges by the number of users and the number of tweets, respectively. In this project, the first 500,000 tweets of the December data from the Stanford tweet dataset were used. When building the edges as described here, there were  121,788,048 edges, 500,000 tweet vertices, and 400,293 user vertices, meaning that the average user was connected to 304.2 tweets and the average tweet was connected to 243.5 users.

There are quite a few duplicate edges created using the above process because checking for duplicates at the time of graph creation slows that down dramatically; it is far easier to either ignore them when running the algorithm or to simply remove them from the graph after it has been created. Of the more than 120 million edges created, only  110,585,315 distinct edges were created. Experiments were performed to determine whether to ignore duplicate edges or whether to leave them in place to act as a weighting factor on the output---those results are discussed in Chapter 5. Additionally, in this graph a very large portion of the edges are content edges: 80,535,848. These were split more or less equally between edges based in hashtags and edges based on named entities:  42,608,235 of the edges were hashtag edges. The speed of the algorithm can be increased dramatically by eliminating these edges, and the results of that on the quality of the recommendations are discussed in Chapter 5.


\section{Initializing Scores}
\label{sec:InitializingScores}

Broadly speaking, the initialization of scores was implemented as described in Section~\ref{sec:InitializationOfScores}. The user scores were initialized according to the method of Adamic and Adar (\cite{Adamic2003}) as described in \cite{LibenNowell2007} and the tweet scores were initialized via cosine similarity.

\subsection{Tweet Scores}
\label{sec:TweetScoresImplementation}

Calculating the initial tweet scores was mostly trivial. For purposes of finding a reference document which indicated the interests of the distinguished user against which all tweets could be compared, several possibilities were used. The first thing checked was whether a particular user had retweeted anything during the time period under study since retweets are the best predictor of what a user is interested in. If they had retweeted any content then those retweets were combined into one document with the `RT' removed and used as the reference document. Unfortunately, given that the data presented was a small subset of the overall data, most users had no retweets and those who did have any usually only had one, which decreases the accuracy of this technique by focusing on one particular interesting tweet.

For those users with no recorded retweets in the timeframe under study, the composite document can be formed using the tweets of all of their followees. These can conveniently be recovered based on the edges by selecting all follower edges which connect to the distinguished user.

The initialization of the tweet scores uses the previously discussed Lucene library to aid with content similarity. Lucene stores term frequency information within its index, simplifying the calculation of tf-idf information for use with cosine similarity. All of the tweet vertices are selected from the database and then iterated through with a tf-idf document vector created for each tweet and then compared to the reference document using cosine similarity. Once all of the scores have been added to the database, they can be normalized to add to one by using the SQL statement shown in Section~\ref{sec:DataStorage}.

The canonical method of tf-idf calculation had an interesting side effect when used with the very short tweet documents. The brevity of the format means that many short words are used and the social nature of the network means that many personal pronouns are used. The canonical method of calculating cosine similarity using tf-idf did not adequately discount these highly common words, resulting in very highly scoring tweets which contained things such as "I love you so much", or which included multiple pronouns, like "I want you I need you I love you".

These tweets clearly had no value on their own, so the decision was made to provide an additional factor to increase the amount by which these words were discounted. To do this, the idf score provided by the canonical tf-idf measurement was modified by multiplying it by an additional factor:

\begin{center}
\[
tf(t) \times idf(t) \times discount(t) = |t| \times \log(\frac{|D|}{df(t)}) \times \frac{1}{\log(df(t) + 1)}
\]
\end{center}

The value of this discount factor would range from 0.175 for a term that occurred in all 500,000 tweets to 3.32 for a term that occurred in only one tweet. This serves to increase the value of rare words while doing a better job of discounting extremely common words and dramatically improved the quality of the initial tweet scores. They still were not overly interesting by themselves, but certainly provided a better starting point than with the canonical tf-idf measurement.


\subsection{User Scores}

Calculating the initial user scores was one of the more time-intensive parts of the process and led to the creation of extra columns in the user\_vertices table to allow all of the users being investigated to have their initial scores stored so that they did not need to be calculated each time a different user was used for experimentation. Clustering the namenetwork table and creating the followercount table were both motivated largely by speed gains while calculating the initial user score.

Recall the scoring method of Adamic and Adar from Section~\ref{sec:InitializationOfScores}. If $\Gamma (x)$ is defined as the set of all connections (neighbours) of vertex $x$, then the similarity score for two users x and y is given as: 

\begin{center}
\[
\sum\nolimits_{z \in \Gamma (x) \cap \Gamma (y)} \frac{1}{\log |\Gamma (z) |}
\]
\end{center}

For initializing the user scores, user $x$ is always the distinguished user and user $y$ is the user currently being scored. In the experiments of both \cite{Adamic2003} and \cite{LibenNowell2007} utilizing this formula the networks under study were non-directed, so the simple metric for the $\Gamma$ function of neighbouring vertices was adequate. In the Twitter network, however, connections are directed.

Clearly it does not make sense to look for the intersection of the followers of two users to determine their similarity, since users have no control over this. A better method would be to look for the intersection of the followees of two users since it would at least indicate that they were interested in seeing the same things. Still, it would not indicate whether one user actually created content of interest to the other user.

For this project, the function used was the intersection of the followees of the distinguished user with the followers of the user being scored. If users that the distinguished user has interest in express interest in another user, then it stands to reason that this other user will be more likely to be of interest to the distinguished user.

For each user $z$ in this intersection, the score component from that user is calculated according to $\frac{1}{\log |\Gamma (z) |}$, and this is summed for all such users $z$. In this formula, $|\Gamma (z)|$ represents the size of the neighbourhood $\Gamma (z)$, i.e. the number of followers that the user $z$ has, as found in the followercount table. This formula discounts the scoring effect of very popular users because they will naturally have more overlap in their follower lists with the followees of the distinguished user.

This process is accomplished in the project by running a SQL query to retrieve the follower counts of all of these users and then summing them in the code. That SQL query is:

\begin{verbatim}
SELECT C.count FROM
  ( (SELECT username AS name FROM namenetwork_followers
            WHERE followername=?)
      INTERSECT
    (SELECT followername AS name FROM namenetwork
            WHERE username=?)
  ) S, followercount C
  WHERE S.name=C.username;
\end{verbatim}

\noindent
where followername is parametrized with the username of the distinguished user and username is parametrized with the username of the user being scored.

The one user for whom this method does not work is the distinguished user---that user's score would not be properly returned using this method. Instead, the distinguished user has their initial score set to some factor of the largest score found before normalization. For this implementation this factor was 1.5, meaning that the distinguished user's initial score before normalization would be set to 1.5 times the maximum score from all other users. This is designed to provide a strong score impact from the things that the distinguished user is connected to since that provides a great deal of information about content and users they are likely to be interested in.


\section{Implementation of the Co-HITS Algorithm}


The iterative version of the Co-HITS algorithm itself can be implemented to run in linear time and space once the graph and its associated indexes have been created. The process of indexing these various fields during the creation of the graph is $\mathcal{O}(n \cdot \log(n))$, but this only needs to be performed one time for the graph and once it is complete any number of users can have their recommendations built off of that graph with only linear time complexity. Furthermore, it is easy to update the graph incrementally if new users or tweets become available.

More specifically, the complexity of the algorithm is $\mathcal{O}(E \cdot I)$, where $E$ is the number of edge vertices and $I$ is the number of iterations. For each iteration the edges are sorted first by the user name, when calculating the tweet scores, and then by the tweet id, when calculating the user scores. This sorting is performed when the tweets are indexed, however, not when running the algorithm. When updating the tweet scores, the following SQL statement is used to select the edges:

\begin{verbatim}
SELECT U.score as user_score, U.name, E.type, T.tweetid
  FROM user_vertices U, edges E, tweet_vertices T
  WHERE T.tweetid=E.tweetid AND E.name=U.name
  GROUP BY U.name, E.type, T.tweetid;
\end{verbatim}

\noindent
For updating the user scores, the following SQL statement is used to select the edges:

\begin{verbatim}
SELECT U.name, E.type, T.score as tweet_score, T.tweetid
  FROM user_vertices U, edges E, tweet_vertices T
  WHERE U.name=E.name AND E.tweetid=T.tweetid
  GROUP BY T.tweetid, E.type, U.name;
\end{verbatim}

\noindent
Note that in both cases the scores of the opposite vertex type from that being updated must be selected so that the update can be performed appropriately. For example, in the case of updating the tweet scores, the user score for each vertex must be known so that the score contribution from that vertex over the edges emanating from it can be calculated.

Each vertex is considered in order and its score contribution to the various nodes on the other side is considered. The sorting makes it easy to determine the total number and type of edges emanating from a particular vertex because all of the edges from that vertex are considered one after another. Knowing the total number of edges leaving a particular vertex is necessary when calculating the transition probability. Upon iterating through all edges sorted by one component (either users or tweets), the scores for the other half of the graph can be updated with the newly calculated values.

Pseudocode of the implementation for one direction, updating the tweet scores, is shown for one iteration in Algorithm~\ref{alg:CoHitsAlgorithm}.

%LaTeX is finnicky, and in many cases, very small changes to this mess the whole thing up
\begin{algorithm}
% \SetAlgoLined
 \SetKwData{LastName}{curUser}
 \SetKwData{Edge}{edge}
 \SetKwData{E}{e}
 \SetKwData{FirstEdge}{first\_edge}
 \SetKwData{CurUserEdges}{curUserEdges}
 \SetKwData{TweetScores}{tweetScores}
 \SetKwData{Chance}{tweetProbability}
 \SetKwData{Score}{scoreEffectFromThisEdge}
 \SetKwData{CurScore}{curScore}
 \TweetScores = $\emptyset$\;
 \LastName $\leftarrow$ \FirstEdge.username\;
 \CurUserEdges = $\emptyset$\;
 \While{database cursor for edges not at end}{
   \Edge $\leftarrow$ edge at current database cursor position\;
   \If{\LastName $\neq$ \Edge.username}{
     \For{edges \E $\in$ \CurUserEdges}{
       \Chance $\leftarrow$ 1 / (size of \CurUserEdges)\;
       \Score $\leftarrow$ \Chance * \LastName.score * $\lambda_{t}$\;
       \eIf{\TweetScores contains key \Edge.tweetid}{ 
         \CurScore $\leftarrow$ \TweetScores.get value for key \Edge.tweetid\;
         \CurScore $\leftarrow$ \CurScore + \Score\;
         \TweetScores.update(\Edge.tweetid , \CurScore)\;
        }{
         \TweetScores.put(\Edge.tweetid , \Score)\;
        }
     }
     clear \CurUserEdges\;
   }
   add \Edge to \CurUserEdges

  \LastName = \Edge.username

  move database cursor to next edge\;
 }
 update the tweet scores in the database according to \TweetScores
 \caption{The Co-HITS Implementation for one iteration of updating the tweet scores. Updating user scores is identical, but with changes to the appropriate variable names.}
 \label{alg:CoHitsAlgorithm}
\end{algorithm}

It is clear from examining Algorithm~\ref{alg:CoHitsAlgorithm} that each edge is actually traversed twice, and since this algorithm is repeated twice for each iteration, the edges are actually traversed 4 times for each iteration. This is an unfortunate consequence of the need to know the number of edges leaving a particular vertex before calculating that vertex's effect on the vertices to which it connects.

When updating the scores, two steps are used. First, the original score is set according to the original\_score column, the $\lambda_{t}$ parameter, and the total score from vertices which did not have an edge in the current direction because of the directionality of the edges. These dead-end vertices still need to be accounted for, so their scores are distributed evenly between all vertices on the other side of the graph. So, before the scores are set for specific vertices, the original score for all vertices can be updated with the following query:

\begin{verbatim}
UPDATE tweet_vertices SET score=(? + (original_score * ?));
\end{verbatim}

\noindent
Here, the first question mark indicates the sum of the scores from all dead-end vertices, and the second represents the $\lambda_{t}$ parameter. After the baseline score is added uniformly (and very quickly), the base score can be augmented with the calculated score factor for only those vertices which have been changed. This is performed as a batch operation using the following SQL query:

\begin{verbatim}
UPDATE tweet_vertices SET score=(score + ?) WHERE tweetid=?;
\end{verbatim}

\noindent
Only the version for updating the tweet scores is shown, but the user score update proceeds in a nearly identical manner.

The Co-HITS algorithm is intended to be run until convergence, i.e. when the score was not updated for either the tweets or the users during a particular iteration. This would require knowing when the update statement did not result in any content updates, which could be accomplished by introducing an additional constraint on the WHERE clause in the above update statement.

In their paper introducing the Co-HITS algorithm, \cite{Deng2009}, Deng, et. al. say that the iterative algorithm generally converges in under 10 iterations. This was found to be generally true in this project, depending on how convergence is defined. 

Using a WHERE statement to detect convergence never resulted in absolute convergence in the sense that no additional vertices were updated, and relaxing the requirement of how close the value had to be in order to determine convergence simply resulted in the normalization of the scores being knocked out of balance and no longer summing to 1 properly. Still, after ten iterations, and usually much fewer, it was clear by examining the values as they changed that the changes that were still occurring were very small, not actually affecting the final outcome or ordering of the tweets or users. For this project, the algorithm was allowed to run for up to ten iterations before terminating.


\section{Retrieving Results}
\label{sec:RetrievingResults}

Retrieving the results of the algorithm is made easy by the structure of the tables within PostgreSQL; it is simply a matter of running a SQL command. For this project it was thought that users could find and become aware of users with a lot of followers very easily. By extension it was also assumed that users could easily find content by these users.

Thus, when retrieving the recommendations, only those whose follower count fell below a certain threshold were considered; users above that threshold were excluded from the results. On a similar note, it made no sense to recommend content that the distinguished user had already seen, so users who the distinguished user already followed were ignored, as were their tweets. It may be reasonable to include tweets from existing followees in the recommendations for some applications (and Twitter's own recommendation system does this) but for evaluation purposes within this project they were ignored.

The SQL statements for selecting the highest scoring tweets is:

\begin{verbatim}
SELECT T.name, T.score, T.tweet FROM
  tweet_vertices U,
  (
     (SELECT T.name FROM tweet_vertices T, followercount F
         WHERE T.name=F.username AND
                     F.count < 100000 ORDER BY T.score DESC)
   EXCEPT
     (SELECT username AS name FROM namenetwork_followers
         WHERE followername=?)
  ) S
  WHERE T.name = S.name ORDER BY T.score DESC;
\end{verbatim}

\noindent
The SQL statement for selecting the highest scoring users is:

\begin{verbatim}
SELECT U.name, U.score FROM
  user_vertices U,
  (
     (SELECT U.name FROM user_vertices U, followercount F
         WHERE U.name=F.username AND
                     F.count < 100000 ORDER BY U.score DESC)
   EXCEPT
     (SELECT username AS name FROM namenetwork_followers
         WHERE followername=?)
  ) S
  WHERE U.name = S.name ORDER BY U.score DESC;
\end{verbatim}

\noindent
These two SQL statements both take 100,000 followers as the threshold of followers below which the content is deemed useful for recommendation purposes, but this can easily be adjusted.

By default this does not filter out two classes of tweets that a user is unlikely to be interested in: his own and those containing @replies to other users. Similarly, for the sake of brevity, the SQL statement for retrieving users does not filter out the distinguished user, though it would be simple to do so. In both cases, these tweets are filtered out before the recommendations are delivered to the user.



\section{Parallelization and Performance Improvement}


Though the speed of the algorithm has not yet been described except in broad theoretical terms, it is quite slow in practice. This is not due to the inherent speed of the algorithm, but rather it owes to the fact that all of the data being used is stored in a series of databases on an external storage device and accessing that data is very slow and must be performed in serial.

The creation of the graph, for example, takes more than 24 hours due to the large number of lookups and insertions that must be done in order to connect more than 120 million edges. To illustrate this point, consider the follower edge type. For each tweet the followers of the author need to be looked up in order to determine the follower edge tweets, and then each of these edges must be inserted into the edges table in the database. The result is a slow process. Running the algorithm itself is much faster, but still on the order of hours rather than minutes or seconds. As with the speed of creating the graph, the slowness in running the algorithm is caused by the slowness of retrieving so much data from the hard disk.

With many of the steps of the algorithm it would be very reasonable to the step in parallel using an algorithm such as MapReduce and thus to dramatically increase the speed of the entire process. If many different external storage devices are used then the speed of initializing the user scores can be increased by running the initialization completely in parallel. The maximum limit of this process would be to have one external storage device for each user in the graph, allowing the initialization to be done in seconds.

The creation of the edges in the graph can also be run in parallel. In the existing algorithm, the creation of edges is done by iterating through each tweet and creating appropriate edges based on the type of tweet (mention, retweet, @reply), which entities and hashtags it contains, and who follows the author. It would be simple to duplicate the database information on multiple external storage devices and to determine the edges for multiple tweets in parallel.

And of course, the algorithm itself could also be run in parallel. Recall that the edges are ordered when they are retrieved for updating the scores for each side of the bipartite graph. The score contributions onto the other side are considered for each vertex, a process made easier by the fact that the edges are are ordered by the vertex being considered. The edges could be split so that these vertices were considered in parallel by being selected from separate external storage devices.

Similarly, for a long-running system it would be possible to build much of the graph in real time as tweets came in, thus amortizing the costs of building it. It would also be possible to keep a matrix that had the similarity scores between all users which could be used when initializing the user scores, one of the longer running steps. Building such a matrix initially would be very time consuming, obviously, but once it was built then it could be updated incrementally at a much smaller time cost.

Unfortunately, this parallelization was not possible for this project due to the large amount of hardware required and the time required to implement it. As such, each run of the algorithm was quite slow, limiting the number of experiments that it was possible to perform. Still, as will be seen in the next chapter, good results were obtained and many experimental variations on those results were run to gain good insight into which parts of this implementation were good and which need to be tweaked in future work in this field.





