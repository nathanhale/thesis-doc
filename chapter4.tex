\chapter{Implementation}

This chapter describes how the method of Chapter 3 was actually implemented for this project, a non-trivial step considering the major difficulties associated with the vast amount of data used. It begins by describing how the dataset was selected and the challenges that the large dataset presented in terms of storing and accessing the data. From there it moves on to describe how the bipartite graph was constructed, including selection of tweets and users, construction of edges,  and initialization of scores. From there, the actual implementation of the algorithm is described both in words and in pseudocode before finally describing the method of retrieving the results from the database.

Java was chosen as the programming language for this project due to the wide adoption of libraries available to interface with it and the ease of development with those libraries. The Lucene information retrieval library\footnote{http://lucene.apache.org/core/}, for example, is originally written in Java and was used to index the tweets in a way that made tf-idf cosine-similarity calculations easy. It also facilitated easy search of the tweet database which was useful during the development process to learn more about the nature of the dataset. In addition to Lucene, the Java Database Connectivity library (JDBC) was used to interact with the PostgreSQL database and the Stanford named-entity recognizer was used for the named entity recognition.

\section{Selecting a Dataset}
\label{sec:SelectingADataset}

%TODO: need to cite the stanford data

When beginning the project, one of the first questions was which dataset to use. There were four possibilities: downloading data via the Twitter API specifically for this project, the Choudhury data (\cite{Choudhury2010}), the Stanford data using the social graph of Kwak et. al. (\cite{Yang2011}, \cite{Kwak2010}), and the database created for the Text Retrieval Conference (TREC). Each had advantages and disadvantages.

Given the availability of existing datasets that would meet the needs of the project, the possibility of downloading a social graph and set of tweets specifically for this project was dismissed early on, leaving the task as one of selecting which of the three datasets was best.

The smallest dataset was the Choudhury data. It consisted of tweets collected between late 2008 and late 2009 and contained more than 10 million tweets and more than 800,000 edges. This database is much smaller than the network was at the time it was created; it contains a tiny fraction of the tweets which were produced during that time and a much smaller fraction of the number of users. It was created by starting a seed set of users and then building the database out from there, but does not include the complete set of followers or followees for any user, though it is a connected graph. For example, a number of users have a list of some followers and then are shown to follow no one. This lack of completeness made this dataset unacceptable for this project.

The second dataset was the TREC dataset\footnote{http://trec.nist.gov/data/tweets/}, consisting of 16 million tweets collected over a two week period in late January and early February of 2011. It is not intended to be a complete list of the tweets from that time period, merely a representative sample which includes spam tweets amongst the important tweets. The major problem with this dataset, however, is that it did not include any social graph information, just the tweets. Since a major portion of this project was based on the impact of the social graph on the recommendations, this clearly was not possible to use.

The Stanford Twitter dataset contains 476 million tweets from more than 17 million users. These tweets were collected over a period of 7 months between 1 June, 2009 and 31 December, 2009. The authors of \cite{Yang2011} estimate that these tweets covered approximately 20-30\% of the tweets published in that time period\footnote{http://snap.stanford.edu/data/twitter7.html}. By itself, this dataset would be unusable for the same reason as the TREC database since it does not include any social graph information. Fortunately the authors link to the published social graph of \cite{Kwak2010}, which was crawled via the Twitter API contemporaneously with the collection of the Stanford tweet data. Though the vastness of this dataset presented many challenges during the project, it also was the best choice for putting together a recommendation system to be as accurate as possible.

%TODO should these two paragraphs be included??
Of these three datasets, only the TREC tweet data and the social graph of \cite{Kwak2010} are currently available to the public. Twitter changed their terms of service, requiring care to be taken with the data and researchers to remove tweets from their databases if they were deleted by their original authors. The TREC data is accompanied by a tool which queries the Twitter service to accomplish this, which is why it remains, though even it requires that the United States National Institute of Standards and Technology issue a password to allow the data to be downloaded.

The tweets from the Stanford dataset were able to be used here because one of the researchers within the Oxford University Department of Computer Science had downloaded it before this change came into effect.


\section{Data Storage}
\label{sec:DataStorage}

Considering the vast amount of data available in the dataset, an efficient data storage mechanism was vital. The major consideration was the ease and efficiency of retrieving the data. PostgreSQL was selected to store the information because it has excellent support and is a mature database management system capable of being used with large amounts of data. It has a good interface with the Java programming language and is also open source, making it possible to tweak the implementations of certain features if necessary, though that was not done here.

The biggest source of data to be put into the PostgreSQL database was the social graph. The tweets took up far more disk space, but not all of them needed to be stored in the database because most of them were not used, as described in Section~\ref{sec:BuildingTheGraph}. The complete social graph, meanwhile, was approximately 24 GB, with each line containing a pair of user ids separated by a tab delimiter. Unfortunately the tweets data from Stanford had each tweet indicated only by the user's name, meaning that the data from the social graph which had only user id pairs had to be translated before being stored.

The final \texttt{namenetwork} table, whose schema is shown in Table~\ref{tab:namenetworkSchema}, used the actual usernames as looked up from another database that accompanied the social graph data. Indices were created around both the username and followername columns, with the username column being clustered since it is used the most frequently and has the most data returned from it since the most popular users have far more followers than followees.

Still, it was helpful for certain queries to have fast access to the followees of a given user, so a second copy of the namenetwork table was created to make these queries faster---this version was called \texttt{namenetwork\_followers} and had the same schema as the namenetwork table but was clustered around the followername field rather than the username field.

The usernames were all converted to lower case, and indeed all usernames which could potentially be parsed in mixed case found while indexing the tweets were converted to lower case. Additionally, the social graph data contained a number of duplicate user names. This happened because the program which aggregated the data apparently ran over an extended period of time over which some user accounts were deleted and re-created. Additionally, a large number of user ids corresponded to the name `n/a'. All of these duplicate names were removed from the database.

\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
username & VARCHAR & The name of a user \\ \hline
followername & VARCHAR & The name of a user who follows the user in the username column for this row \\
\end{tabular}
\caption{Schema of the `namenetwork' table}
\label{tab:namenetworkSchema}
\end{table}


For the initialization of the user scores (cf. Section~\ref{sec:InitializingScores}) using the method of Adamic and Adair (cf. Section~\ref{sec:InitializationOfUserScores}) it was necessary to know how many followers each user had. This could be calculated on the fly by running a simple SQL \texttt{COUNT()} query on the namenetwork table, but this would sacrifice performance, so it made more sense to do this query once and create a table for it. The \texttt{followercount} table, whose schema is shown in Table~\ref{tab:followercountSchema}, was created using the following SQL command:

\begin{verbatim}
  CREATE TABLE followercount AS
      (SELECT username, COUNT(followername) AS count
          FROM namenetwork GROUP BY username);
\end{verbatim}

\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
username & VARCHAR & The name of a user \\ \hline
count & INT & The number of followers of the user in the username column for this row \\
\end{tabular}
\caption{Schema of the `followercount' table}
\label{tab:followercountSchema}
\end{table}

\noindent
A hash index was created on the username column of the followercount table to allow for quickly looking up the number of followers that a user has.

%TODO: check how much time 500k tweets represents!
The tweets themselves were not all used in any given run of the algorithm, and in fact only a tiny portion of them were used. One guideline for the project was that the recommendations themselves be only for a particular time period on the order of one to two weeks in order to reflect fleeting interests and recommend things that are of interest to the user at that time. For example, during the Olympics someone may be very interested in tweets about the Olympics, but they would probably not be so interested a few weeks after the Olympics have ended. For this project one million tweets were indexed when the algorithm was initialized, corresponding to about 1 week's worth of the data. Considering that the full dataset contains nearly half a billion tweets, this is a very small portion.

It was useful to store this small portion of tweets in two different ways for two different purposes. For purposes of initializing the tweet scores (again, cf. Section~\ref{sec:InitializingScores}) it was very useful to index the tweets in a Lucene index because such indices automatically count the frequency of each term in each document and in the document collection as a whole, making tf-idf cosine similarity scoring easy.

At the same time, it was also valuable to be able to write SQL statements to quickly select particular tweets and to update their Co-HITS scores quickly, a task much better accomplished via a relational database such as PostgreSQL. This storage of the tweets represented the tweet vertices in the graph, and thus information was included about the Co-HITS score and the original Co-HITS score for each of these nodes. The schema for the \texttt{tweet\_vertices} table is shown in Table~\ref{tab:tweetverticesSchema}. This table was indexed on the tweetid field using a btree index. After the initial index was built, an operation which only needed to happen one time for the series of tweets under investigation, the index was clustered.

\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
tweetid & INT & The unique id of the tweet that this vertex represents \\ \hline
name & VARCHAR & The author of the tweet that this vertex represents \\ \hline
date & TIMESTAMP  & The timestamp of when the tweet that this vertex represents was published \\ \hline
tweet & VARCHAR & The text of the tweet that this vertex represents \\ \hline
score & FLOAT & The Co-HITS score for this tweet \\ \hline
original\_score & FLOAT & The original Co-HITS score for this tweet before the algorithm's first iteration. \\
\end{tabular}
\caption{Schema of the `tweet\_vertices' table}
\label{tab:tweetverticesSchema}
\end{table}

The tweets were stored in the database, so it obviously makes sense that the user vertices would be stored in the database as well. The schema for the \texttt{user\_vertices} table is shown in Table~\ref{tab:userverticesSchema}. Similar information needed to be tracked with these vertices as with the tweet vertices, most notably the current Co-HITS score and the original Co-HITS score. The index on this table was a btree index on the name field, and as with the tweet vertices this index was clustered after it was populated since this only needed to happen one time.

\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
name & VARCHAR & The name of the user that this vertex represents \\ \hline
score & FLOAT & The Co-HITS score for this user \\ \hline
original\_score & FLOAT & The original Co-HITS score for this user before the algorithm's first iteration. \\
\end{tabular}
\caption{Schema of the `user\_vertices' table}
\label{tab:userverticesSchema}
\end{table}

Storage in a PostgreSQL database facilitated some easy implementations of certain operations. For example, by storing the original score in the vertex it was possible to quickly determine whether the original scores had been initialized and normalized by simply checking whether the sum of the original\_score column was equal to one:

\begin{verbatim}
SELECT SUM(original_score) FROM user_vertices;
\end{verbatim}

\noindent
Similarly, the scores could be normalized with only one SQL command:

\begin{verbatim}
UPDATE tweet_vertices
    SET original_score=(original_score / S.sum)
    FROM (SELECT SUM(original_score) FROM tweet_vertices) S;
\end{verbatim}

\noindent
And if the algorithm needed to be reset so that it could be run again, it was a simple matter of copying over the original\_score field to the score field. In the actual implementation, three additional columns were present in the two vertex tables, one for each of the distinguished users. These columns stored the original scores for each of the users and made it possible to calculate this value only once for each user, facilitating faster testing.

The final component that was stored in the PostgreSQL database was the list of edges between the two different classes of vertices. The schema of the \texttt{edges} table is shown in Table~\ref{tab:edgesSchema} and is quite basic. The name of the user and the id of the tweet that are connected is shown, along with the type of the edge. The directionality (if any) of that edge is indicated purely by its type, but by default all edges were treated as non-directed. Experimentation with making edges of particular types directed is addressed in Section~\ref{sec:VaryingEdgeDirectionality}.


\begin{table}
\centering
\begin{tabular}{l|c|p{8cm}}
{\bf Field Name} & {\bf Field Type} & {\bf Description} \\ \hline
name & VARCHAR & The name of the user that this edge connects to. \\ \hline
tweetid & INTEGER & The unique identifier of the tweet that this edge connects to. \\ \hline
type & INTEGER & The type of edge that this represents (see Table~\ref{tab:EdgeTypes}). \\
\end{tabular}
\caption{Schema of the `edges' table}
\label{tab:edgesSchema}
\end{table}

\section{Building the Graph}
\label{sec:BuildingTheGraph}

Building the graph is a very important part of the process. This was taken care of in the GraphManager class (cf. Appendix~\ref{app:code}), and in particular in the \texttt{createGraph()} method and the other methods called from there. First the users and tweets are selected and indexed and then all the appropriate edges between the two sides of the bipartite graph are created.

\subsection{Selecting Users and Tweets}

How to build the graph itself is not as simple a question as it would seem to be at first. Which users should be present in the graph? Which tweets should be present? How the graph is created and the decisions about which nodes are present dictates how connected the graph will be and thus how the scores from each set of vertices will interact with one another. Having users who are not connected to the rest of graph serves no purpose because no information about their suitability can be gained.

Tweets were added by opening one of the files containing tweet data and parsing until the appropriate number of tweets had been indexed. Certain tweets in the data were invalid and were thrown out. The tweet ids were created by beginning at 1 and then incrementing for each tweet that was added to the database (i.e. each valid tweet). Proceeding in this manner ensured that the tweets were from the same time period and had a unique identifier.

In order to ensure that the graph was maximally connected, only users who would have connections were added. Practically speaking, this meant that users were added as the tweets were placed into the tweet\_vertices database. All users who had authored a tweet were added, but so were all users who had been retweeted or mentioned. Because of the types of edges created, as described in Section~\ref{sec:EdgeCreation}, these users are guaranteed to at least be connected to one tweet and will likely be connected to several others.

%TODO: fill in these numbers:
The average degree of a user node and a tweet node can be taken by dividing the number of edges by the number of users and the number of tweets, respectively. For the set of tweet data used in this project, the first 500,000 tweets of the December data from the Stanford tweet dataset, there were \_\_\_ edges, \_\_\_ tweet vertices, and \_\_\_ user vertices, meaning that the average user was connected to \_\_ tweets and the average tweet was connected to \_\_ users.


\subsection{Edge Creation}
\label{sec:EdgeCreation}

%TODO: discuss duplicate edges and also figure out what I want to do about them -- I think I want to leave them in and deal with them at the time of the algorithm...

Once the users and tweets have been added to the database, the next step is to add the edges between the users and the tweets. There were twelve different types of edges used in this project, though the list is by no means exhaustive of the possibilities. The different edge types are listed in Table~\ref{tab:EdgeTypes}.

The authorship edges are simple and are actually created when the users and tweets are initially indexed. For each tweet added to the database its author is added if that user is not already present and then an edge is added between the tweet and the author. These edges are simple.

The remainder of the edge types are more complicated and fall broadly into two categories. The first category is the follower, retweet, mention, and @reply edges, which correspond to the network edges connecting users to other users and the edges connecting users to particular tweets. The second category of edge is the content edges which correspond to the edges between tweets.


\begin{table}
\centering
\begin{tabular}{p{1.9cm}|p{10cm}}
{\bf Edge Type} & {\bf Description} \\ \hline
Authorship & Connects a user to a tweet authored by that user \\ \hline
Follower & Connects a user to a tweet authored by one of their followees, as described in Section~\ref{sec:ProjectingToBipartite} \\ \hline
Retweet & Connects the tweet to the person being retweeted \\ \hline
Retweet Followees & Connects the tweets of the followees of the person being retweeted to the retweeter \\ \hline
Retweet Followers & Connects the tweets of the person being retweeted to the followers of the retweeter \\ \hline
Mention & Connects the tweet to the person being mentioned \\ \hline
Mention Followees & Connects the tweets of the followees of the person being mentioned to the mentioner \\ \hline
Mention Followers & Connects the tweets of the person being mentioned to the followers of the mentioner \\ \hline
At Reply & Connects the tweet to the person being @replied to \\ \hline
At Reply Content & Connects the tweets of the person being @replied to to the tweeter \\ \hline
Hashtag & Connects all tweets which use a given hashtag to the authors of those tweets \\ \hline
Content & Connects all tweets discussing similar content (cf. Section~\ref{sec:DeterminingContentSimilarity}) to the authors of those tweets. \\
\end{tabular}
\caption{The types of edges in the graph}
\label{tab:EdgeTypes}
\end{table}


\subsubsection{Network Edges}

The follower edges are the most straightforward to describe and are simply the result of following the procedure for projection of user connections described in Section~\ref{sec:ProjectingToBipartite}.

The retweet and mention edges are more complicated but are very analogous to one another. The simplest of these edges is the simple retweet or mention edge which connects the tweet to the person being mentioned or retweeted; as with the authorship edges, these are created at the time that the tweets are initially indexed. These edges capture the intuition that a person is likely to be interested in a tweet that mentions them or comments on something they say. The followees and followers versions of the retweet and mention edges, meanwhile, are more complicated.

The followees edge creates a connection between the person who is retweeting or mentioning someone and the followees of the person being mentioned or retweeted. This is because that user has proven by their retweet or mention that they are very interested in content from the user who authored the original tweet, so it stands to reason that they are correspondingly more likely to be interested in the same things as that person.

The followers edge connects the tweets of the person who was retweeted or mentioned to all of the followers of the person who retweeted or mentioned them. If a user is interested enough in someone to follow them and see their tweets then it stands to reason that they would also be interested in someone that their followee is very interested in.

The @reply edges are quite straightforward, though they probably provide less impact on the final rankings because they generally only reinforce the existing follower and followee connections. These edges connect the tweet to the person the @reply was directed at and connect the tweets of the person the @reply was directed towards to the person who sent it.


\subsubsection{Content Edges}

There are far fewer content edges, but they convey a great deal of information. These edges are based on the system of projecting edges between tweets into the bipartite graph by connecting the edges to users instead using the procedure detailed in Section~\ref{sec:ProjectingToBipartite}.

Hashtag edges represent the connections between tweets using the same hashtag. These edges connect all the tweets using a particular hashtag to all of the authors of tweets containing those hashtags. Figure~\ref{fig:content_projection} demonstrates this process. The hashtags are converted to lowercase for normalization purposes and because Twitter itself treats hashtags as being case insensitive.

The content edges connect tweets in the same way, but using the implicit links between tweets with similar content as the basis for the projection. How the similarity in content is determined can vary between implementations and range from something complicated such as Latent Dirichlet Allocation topic models to something much simpler.

\subsection{Determining Content Similarity}
\label{sec:DeterminingContentSimilarity}

Determining which tweets contain similar content to one another is an important part of the algorithm used here because it provides many connections between users and tweets that are not connected via straightforward network connections, allowing novel tweets to be recommended by the algorithm.

The method for determining this similarity need not be very complicated, though it certainly can be. Using topic models and other techniques from statistics and machine learning would certainly generate good results for determining content similarity, but at the cost of being quite computationally expensive.

For this project a much simpler and faster method was used: named-entity recognition (NER). Using the Stanford Named-Entity Recognizer \cite{StanfordNER2011}, each tweet was run through the tool in order to find any named entities such as people, places, and organizations. This was done as the tweets were being processed for other content information such as hashtags.

Each named entity found was converted to lower case and stored in memory along with the id of the tweet in which it appeared. If another tweet was found containing the same named entity then that tweet's id was stored along with the ids of the other tweets already found with that id. Once all of the tweets had been processed, each list of tweetids was connected to the authors of the respective tweets.

This process is far from perfect, but provides a good approximation of the people, places, and things that users are talking about. In experiments done prior to incorporating this tool into the algorithm, the NER of the Stanford library was found to be generally accurate, despite the lack of context provided by the space constraints of tweets.

The biggest problem with the use of the recognizer in this algorithm is that it returns compound entities as separate entities, meaning that entities such as `United Kingdom' would be returned as two separate words. Similarly, names when given as first and last names (e.g. `David Beckham') would be returned in two chunks. This presents a question as to whether these words should be treated individually or combined into one entity.

Leaving them as separate words gives some poor results such as `David' or `United' being counted as entities even though they are not very useful, while combining them separates cases where people are referred to by last name, meaning that `Beckham' and `David Beckham' would be two different entities even though they refer to the same thing. Similarly, phrases such as `David Beckham, United Kingdom' would be returned as one big entity, which is clearly nonsensical.

For the purposes of this project, the former approach of leaving all entities as separate was taken. This leads to a small number of garbage edges but with the benefit of more correct connections and a simpler implementation. Put another way, this choice increases the recall of the connections between content while decreasing the precision.


\section{Initializing Scores}
\label{sec:InitializingScores}

Broadly speaking, the initialization of scores was implemented as described in Section~\ref{sec:InitializationOfScores}. The user scores were initialized according to the method of Adamic and Adar (\cite{Adamic2003}) as described in \cite{LibenNowell2007} and the tweet scores were initialized via cosine similarity.

\subsection{Tweet Scores}

Calculating the initial tweet scores was mostly trivial. For purposes of finding a reference document which indicated the interests of the distinguished user against which all tweets could be compared, several possibilities were used. The first thing checked was whether a particular user had retweeted anything during the time period under study since retweets are the best predictor of what a user is interested in. If they had retweeted any content then those retweets were combined into one document with the`RT' removed and used as the reference document. Unfortunately, given that the data presented was a small subset of the overall data, most users had no retweets and those who did have any usually only had one, which decreases the accuracy of this technique by focusing on one particular interesting tweet.

For those users with no recorded retweets in the timeframe under study, the composite document can be formed using the tweets of all of their followees. These can conveniently be recovered based on the edges by selecting all follower edges which connect to the distinguished user.

The initialization of the tweet scores uses the previously discussed Lucene library to aid with content similarity. Lucene stores term frequency information within its index, simplifying the calculation of tf-idf information for use with cosine similarity. All of the tweet vertices are selected from the database and then iterated through with a tf-idf document vector created for each tweet and then compared to the reference document using cosine similarity. Once all of the scores have been added to the database, they can be normalized to add to one by using the SQL statement shown in Section~\ref{sec:DataStorage}. 

\subsection{User Scores}

Calculating the initial user scores was one of the more time-intensive parts of the process and led to the creation of extra columns in the user\_vertices table to allow all of the users being investigated to have their initial scores stored so that they did not need to be calculated each time a different user was used for experimentation. Clustering the namenetwork table and creating the followercount table were both motivated largely by speed gains while calculating the initial user score.

Recall the scoring method of Adamic and Adar from Section~\ref{sec:InitializationOfScores}. If $\Gamma (x)$ is defined as the set of all connections (neighbours) of vertex $x$, then the similarity score for two users x and y is given as: 

\begin{center}
\[
\sum\nolimits_{z \in \Gamma (x) \cap \Gamma (y)} \frac{1}{\log |\Gamma (z) |}
\]
\end{center}

For initializing the user scores, user $x$ is always the distinguished user and user $y$ is the user currently being scored. In the experiments of both \cite{Adamic2003} and \cite{LibenNowell2007} utilizing this formula the networks under study were non-directed, so the simple metric for the $\Gamma$ function of neighbouring vertices was adequate. In the Twitter network, however, connections are directed.

Clearly it does not make sense to look for the intersection of the followers of two users to determine their similarity, since users have no control over this. A better method would be to look for the intersection of the followees of two users since it would at least indicate that they were interested in seeing the same things. Still, it would not indicate whether one user actually created content of interest to the other user.

For this project, the function used was the intersection of the followees of the distinguished user with the followers of the user being scored. If users that the distinguished user has interest in express interest in another user, then it stands to reason that that user will be more likely to be of interest to the distinguished user.

For each user $z$ in this intersection, the score component from that user is calculated according to $\frac{1}{\log |\Gamma (z) |}$, and this is summed for all such users $z$. In this formula, $|\Gamma (z)|$ represents the size of the neighbourhood $\Gamma (z)$, i.e. the number of followers that the user $z$ has, as found in the followercount table. This formula discounts the scoring effect of very popular users because they will naturally have more overlap in their follower lists with the followees of the distinguished user.

%TODO: I'd still like to try this without the log

This process is accomplished in the project by running a SQL query to retrieve the follower counts of all of these users and then summing them in the code. That SQL query is:

\begin{verbatim}
SELECT C.count FROM
  ( (SELECT username AS name FROM namenetwork_followers
            WHERE followername=?)
      INTERSECT
    (SELECT followername AS name FROM namenetwork
            WHERE username=?)
  ) S, followercount C
  WHERE S.name=C.username;
\end{verbatim}

\noindent
where followername is parametrized with the username of the distinguished user and username is parametrized with the username of the user being scored.

The one user for whom this method does not work is the distinguished user---that user's score would not be properly returned using this method. Instead, the distinguished user has their initial score set to some factor of the largest score found before normalization. For this implementation this factor was 1.25, meaning that the distinguished user's initial score before normalization would be set to 1.25 times the highest score from any other user. This is designed to provide a strong score impact from the things that the distinguished user is connected to since that provides a great deal of information about content and users they are likely to be interested in.


\section{Implementation of the Co-HITS Algorithm}


The iterative version of the Co-HITS algorithm itself can be implemented to run in linear time and space once the graph and its associated indexes have been created. The process of indexing these various fields during the creation of the graph is $\mathcal{O}(n \cdot \log(n))$, but this only needs to be performed one time for the graph and once it is complete any number of users can have their recommendations built off of that graph with only linear time complexity. Furthermore, it is easy to update the graph incrementally as new users or tweets are added.

More specifically, the complexity of the algorithm is $\mathcal{O}(E \cdot I)$, where $E$ is the number of edge vertices and $I$ is the number of iterations. For each iteration the edges are sorted first by the user name, when calculating the tweet scores, and then by the tweet id, when calculating the user scores. This sort is performed when the tweets are indexed, however, not when running the algorithm. When updating the tweet scores, the following SQL statement is used to select the edges:

\begin{verbatim}
SELECT U.score as user_score, U.name, E.type, T.tweetid
  FROM user_vertices U, edges E, tweet_vertices T
  WHERE T.tweetid=E.tweetid AND E.name=U.name
  GROUP BY U.name, E.type, T.tweetid;
\end{verbatim}

\noindent
For updating the user scores, the following SQL statement is used to select the edges:

\begin{verbatim}
SELECT U.name, E.type, T.score as tweet_score, T.tweetid
  FROM user_vertices U, edges E, tweet_vertices T
  WHERE U.name=E.name AND E.tweetid=T.tweetid
  GROUP BY T.tweetid, E.type, U.name;
\end{verbatim}

\noindent
Note that in both cases the scores of the opposite vertex type from that being updated must be selected so that the update can be performed appropriately. For example, in the case of updating the tweet scores, the user score for each vertex must be known so that the score contribution from that vertex over the edges emanating from it can be calculated.

Each vertex is considered in order and its score contribution to the various nodes on the other side is considered. The sorting makes it easy to determine the total number and type of edges emanating from a particular vertex because all of the edges from that vertex are considered one after another. Knowing the total number of edges leaving a particular vertex is necessary when calculating the transition probability. Upon iterating through all edges sorted by one component (either users or tweets), the scores for the other half of the graph can be updated with the newly calculated values.

Pseudocode of the implementation is shown in Algorithm~\ref{alg:CoHitsAlgorithm}, while the actual implementation is shown in the code listing in Appendix~\ref{app:code} in the \texttt{updateTweetScores()} and \texttt{updateUserScores()} methods of the GraphManager class.

%LaTeX is finnicky, and in many cases, very small changes to this mess the whole thing up
\begin{algorithm}
% \SetAlgoLined
 \SetKwData{LastName}{curUser}
 \SetKwData{Edge}{edge}
 \SetKwData{E}{e}
 \SetKwData{FirstEdge}{first\_edge}
 \SetKwData{CurUserEdges}{curUserEdges}
 \SetKwData{TweetScores}{tweetScores}
 \SetKwData{Chance}{tweetProbability}
 \SetKwData{Score}{scoreEffectFromThisEdge}
 \SetKwData{CurScore}{curScore}
 \TweetScores = $\emptyset$\;
 \LastName $\leftarrow$ \FirstEdge.username\;
 \CurUserEdges = $\emptyset$\;
 \While{database cursor for edges not at end}{
   \Edge $\leftarrow$ edge at current database cursor position\;
   \If{\LastName $\neq$ \Edge.username}{
     \For{edges \E $\in$ \CurUserEdges}{
       \Chance $\leftarrow$ 1 / (size of \CurUserEdges)\;
       \Score $\leftarrow$ \Chance * \LastName.score * $\lambda_{t}$\;
       \eIf{\TweetScores contains key \Edge.tweetid}{ 
         \CurScore $\leftarrow$ \TweetScores.get value for key \Edge.tweetid\;
         \CurScore $\leftarrow$ \CurScore + \Score\;
         \TweetScores.update(\Edge.tweetid , \CurScore)\;
        }{
         \TweetScores.put(\Edge.tweetid , \Score)\;
        }
     }
     clear \CurUserEdges\;
   }
   add \Edge to \CurUserEdges

  \LastName = \Edge.username

  move database cursor to next edge\;
 }
 update the tweet scores in the database according to \TweetScores
 \caption{The Co-HITS Implementation for updating the tweet scores. Updating user scores is identical, but with changes to the appropriate variable names.}
 \label{alg:CoHitsAlgorithm}
\end{algorithm}

It is clear from examining Algorithm~\ref{alg:CoHitsAlgorithm} that each edge is actually traversed twice, and since this algorithm is repeated twice for each iteration, the edges are actually traversed 4 times for each iteration. This is an unfortunate consequence of the need to know the number of edges leaving a particular vertex before calculating that vertex's effect on the vertices to which it connects.

When updating the scores, the following SQL query is used:

\begin{verbatim}
UPDATE tweet_vertices
  SET score=(? + (original_score * ?))
  WHERE tweetid=? AND ABS(score - (? + (original_score * ?))) > 0;
\end{verbatim}

\noindent
Only the version for updating the tweet scores is shown, but the user score update proceeds in a nearly identical manner. The two question marks in the SET clause and in the ABS() portion of the WHERE clause have identical meanings: the first is the score contribution from the other vertices as calculated by Algorithm~\ref{alg:CoHitsAlgorithm}, and the second is the appropriate $\lambda$ value as discussed in Section~\ref{sec:CoHitsParameters}---$\lambda_{t}$ for updating the tweet vertices and $\lambda_{u}$ for updating the user vertices.

The part of the WHERE clause with the check of the absolute value is intended to help with determining whether the algorithm has converged. The query will indicate that no rows were updated if the score does not change, making it easy to determine convergence. In their paper introducing the Co-HITS algorithm, \cite{Deng2009}, Deng, et. al. say that the iterative algorithm generally converges in under 10 iterations. This was found to be generally true in this project, depending on how it was defined.

Using the above SQL statement never resulted in absolute convergence in the sense that no additional vertices were updated, and relaxing the requirement of how close the value had to be in order to determine convergence simply resulted in the normalization of the scores being knocked out of balance. Still, after ten iterations, and usually much fewer, it was clear by examining the values as they changed that the changes that were still occurring were very small, not actually affecting the final outcome. For this project, the algorithm was allowed to run for up to fifteen iterations before terminating.

Updating the scores in this manner also adds an additional complexity factor. It would be possible to perform this in $\mathcal{O}(U)$ or $\mathcal{O}(T)$ time (for users and tweets, respectively) if the vertices were traversed in order and updated in that way, but in practice this is slower than performing a series of batch updates using the B-Tree index which runs in $\mathcal{O}(U \log U)$ time. 


\section{Retrieving Results}
\label{sec:RetrievingResults}

Retrieving the results of the algorithm is made easy by the structure of the tables within PostgreSQL; it is simply a matter of running a SQL command. For this project it was thought that users could find and become aware of users with a lot of followers very easily. By extension it was also assumed that users could easily find content by these users.

Thus, when retrieving the recommendations, only those whose follower count fell below a certain threshold were considered. Similarly it made no sense to recommend content that the user had already seen, users who the distinguished user already followed were ignored, as were their tweets. It may be reasonable to include tweets from existing followees in the recommendations for some applications (and Twitter's own new recommendation system does this) but for evaluation purposes within this project they were ignored.

The SQL statements for selecting the highest scoring tweets is:

\begin{verbatim}
SELECT T.name, T.score, T.tweet FROM
  tweet_vertices U,
  (
     (SELECT T.name FROM tweet_vertices T, followercount F
         WHERE T.name=F.username AND
                     F.count < 100000 ORDER BY T.score DESC)
   EXCEPT
     (SELECT username AS name FROM namenetwork_followers
         WHERE followername=?)
  ) S
  WHERE T.name = S.name ORDER BY T.score DESC;
\end{verbatim}

\noindent
The SQL statement for selecting the highest scoring users is:

\begin{verbatim}
SELECT U.name, U.score FROM
  user_vertices U,
  (
     (SELECT U.name FROM user_vertices U, followercount F
         WHERE U.name=F.username AND
                     F.count < 100000 ORDER BY U.score DESC)
   EXCEPT
     (SELECT username AS name FROM namenetwork_followers
         WHERE followername=?)
  ) S
  WHERE U.name = S.name ORDER BY U.score DESC;
\end{verbatim}

\noindent
These two SQL statements both take 100,000 followers as the threshold of followers below which the content is deemed useful for recommendation purposes, but this can easily be adjusted.

By default this does not filter out two classes of tweets that a user is unlikely to be interested in: his own and those containing @replies to other users. Similarly, for the sake of brevity, the SQL statement for retrieving users does not filter out the distinguished user, though it would be simple to do so. In both cases, these tweets are filtered out before the recommendations are delivered to the user.




