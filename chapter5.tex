\chapter{Results}

Having detailed both the method used for recommendation and the actual implementation of that method, the discussion now moves to the method for evaluating the results and the presentation of the results themselves in order to demonstrate the efficacy of the method.

First the method of evaluating the results is described; for such a large dataset this is a challenging question, and it is not yet well-defined for this research area. The selection of the distinguished users is then described and their effect on the evaluation methods is considered before arriving at several metrics for evaluating the results.

Next the baseline results are shown using the established metrics. Finally, variations on the algorithm are experimented with and their results presented in order to establish which parameters and edge types are the most valuable for this task and whether the algorithm can be improved over its baseline result.


\section{Evaluation Methodology}

\subsection{Challenges}

A review of the existing research on content recommendation discussed in Section~\ref{sec:ContentRecommendationResearch}, reveals no consensus on how best to evaluate the results of a recommendation method. This is due primarily to two separate but related issues: the lack of a common dataset and the lack of ground truth judgements on the relevance of either content or users.

None of the research projects on Twitter content recommendation reviewed in Section~\ref{sec:ContentRecommendationResearch} used the same set of Twitter data for their experiments. All of the researchers used the Twitter API to download their own datasets of varying sizes and using various procedures. Because of this the datasets turn out to be very different in terms of size, user composition, and tweet content, which makes results difficult to compare between papers. The creation of the microblog track of the Text Retrieval Conference (TREC) was accompanied by a very large collection of tweet data which may be commonly used in the future, but as discussed in Section~\ref{sec:SelectingADataset}, it was not sufficient for this project.

The bigger issue, however, is the lack of a set of ground truth judgements of the relevance of tweets and users in any dataset. One obvious cause of this is the lack of a canonical dataset upon which recommendations are made. Perhaps if ground truth judgements were available such a dataset would come into common usage, but the fact is that the job is too complex to realistically be performed reliably for any decent sample size. The content and users that are relevant for one user would be vastly different from the content and users relevant for another. And any database of a usable size would have millions and millions of tweets and users, far too many for all of them to be judged for even one user.


\subsection{Distinguished User Selection}

With these challenges in mind, it was necessary to develop methods of evaluating the performance of the algorithm that did not rely on a pre-judged set of relevant content and users. The first step was to choose distinguished users to whom the content should be recommended. For privacy reasons, those users are not named here. Overall, three users were chosen.

Two users were chosen because they are known personally by the author and are long-time users who are present in the dataset. These facts made them ideal candidates for the user studies discussed in Section~\ref{sec:UserStudy}. Both of these users are computer scientists, so technology is a major interest, but there are some distinguishing interests as well. These two users will be known as users $K_{i}$ and $K_{j}$, with the choice of K meant to indicate that they are the users {\bf K}nown to the author. Both users joined Twitter in early 2009. 

The other user was chosen because his interests are clear and easy to distinguish and because he was well connected to the small sample of tweets that were used during initial implementation. This user has more than 7,000 followers as of this writing, up from approximately 3,700 three years ago when the dataset was collected. As of this writing he has published nearly 25,000 tweets. His interests are also in technology, but with a business focus. A large number of his current followees are technology entrepreneurs, but he also posts tweets and has followees related to popular culture such as films, television, and music. This user will be known as user $U$, indicating that he is {\bf U}nknown to the author.



\subsection{User Study}
\label{sec:UserStudy}

The goal of the user study is primarily to determine whether the content recommended to the users was of interest to the distinguished user.

A user study is perhaps the closest method of evaluation to a set of ground truth relevance judgements, though on a smaller scale. For the tweets ranked by the users it is possible to say with certainty how relevant they are and thus to evaluate the precision and recall of the algorithm. The user study could obviously only be performed on the users known to the author, which was part of what motivated the choice of known users as distinguished users.

One major drawback of the user study is that it recommends content to these users based on what was happening three years ago. Thus, the recommendations are based on the interests of three years ago which may no longer be relevant. Before filling out the user survey these users were asked to put themselves in the frame of reference of what might have been interesting to them three years ago, but this is obviously inexact, so the efficacy of user study depends largely on the idea that interests change slowly.

The user studies consisted of two components: recommending users and recommending individual tweets. 

For recommending individual tweets, the top thirty tweets from each experiment that was run, retrieved as described in Section~\ref{sec:RetrievingResults}, were retrieved after the algorithm was run. Crucially, tweets which were an @reply to another user were filtered out since a user would not see those if they were to follow the user in question. After running this procedure for all of the experiments, there were approximately 150 tweets for each user to rank.

These filtered tweets were then presented to the user in a random order and each user was asked to rank each of the one hundred tweets from 1 to 5 using the values from Table~\ref{tab:UserRankingScoresForTweets}. From these scores, the precision was evaluated by considering tweets scored as either 4 or 5 to be relevant tweets and seeing how many of the top 20 tweets were relevant. Recall is not really possible to evaluate with a currently existing Twitter dataset, unfortunately, because of the lack of ground truth relevance judgements on the Twitter data.


\begin{table}
\centering
\begin{tabular}{c|l}
{\bf Score} & {\bf Description of tweets with this score} \\ \hline
1 & Not relevant at all, e.g. non-English tweets \\ \hline
2 & Useless tweets \\ \hline
3 & Average tweet; not particularly useful, but not completely without value \\ \hline
4 & Relevant or interesting tweet \\ \hline
5 & Very relevant tweets \\
\end{tabular}
\caption{User ranking scores for tweets}
\label{tab:UserRankingScoresForTweets}
\end{table}


Recommending users proceeded in a largely similar manner. A surprising number of the recommended users either no longer appear as users or have set their Twitter feeds to be private, preventing effective evaluation, so the option for the users to rate the user as not available was added. These users were then filtered out of the results when calculating precision.

For each recommended user, the distinguished user under study was presented with a link to their Twitter page so that their profile and recent tweets could be viewed. Each user to be ranked was presented in random order. The users under study were then asked to rank the recommended users according the scale of Table~\ref{tab:UserRankingScoresForUsers}, which is largely similar to the scale for ranking tweets. The results of this ranking were then used in the same way to determine precision scores by considering users scored as either 4 or 5 to be relevant users.

As with the tweets, it was not really possible to evaluate the recall of current users because it is not possible to say how many total relevant users there are. Additionally, since the time requirements for ranking users are higher, only 30 users were presented to the distinguished users to rank.


\begin{table}
\centering
\begin{tabular}{c|l}
{\bf Score} & {\bf Description of users with this score} \\ \hline
0 & Users whose Twitter feeds are no longer available \\ \hline
1 & Not relevant at all, e.g. users tweeting in another language \\ \hline
2 & Uninteresting users \\ \hline
3 & Average user; not particularly interesting, but not without value \\ \hline
4 & Interesting users;  includes users who once were followed but no longer are \\ \hline
5 & Users that the distinguished user now follows \\
\end{tabular}
\caption{User ranking scores for users}
\label{tab:UserRankingScoresForUsers}
\end{table}


\subsection{Comparison to Current Tweets}
\label{sec:ComparisonToCurrentTweets}

The user study is effective for evaluating the results for the known users but is subject to biases and changes in interests and does nothing for evaluating the results for unknown or unavailable users. As such, it was also necessary to develop more automated techniques.

Perhaps the best possibility would be to use the closest thing to a ground truth dataset that exists: the list of all the tweets that a particular distinguished user retweeted. If the user's own connections to each of these tweets were severed then it would be possible to test just how many of these tweets were actually recommended. This presents two major problems, however. First, because the dataset is limited to only approximately 20\%-30\% of the tweets that were published in the time period, many of the initial tweets were missed and only their retweets were captured. Second, in the small time period being examined, most distinguished users would be unlikely to retweet even one or two tweets, leaving not enough data to evaluate.

A more realistic technique is to assume that a user's interests remain static between the time period represented by the dataset and the present day. This makes it possible to compare the tweets recommended by the system to the tweets that the user is actually interested in while using a separate data source to avoid overfitting.

As described by \cite{Welch2011} and mentioned elsewhere in this dissertation, retweets are the most effective means of determining which content a person is interested in. Thus, each tweet recommended by the system can be compared to each of the most recent retweets from the present day twitter stream of the user for whom the recommendations are being created, and the score can be averaged together. These numbers can be compared to a baseline created by comparing each tweet in the system to the reference tweets and taking an average of their similarity scores. This process can be repeated for tweets that the distinguished user has authored.

%TODO: want to reference the modified method, not sure if this reference is the same
For this project, the method of determining similarity was the cosine similarity metric as described in Section~\ref{sec:ContentScoringMethod}. Because of the large number of tweets in the database the baseline against which these numbers were compared was determined by taking the average of the similarity scores for 2,500 tweets rather than for the entire collection. Looking at the similarity scores for each 500 tweets showed that the average scores did not change very much from the score after evaluating the first 500, validating the choice of 2,500 as the limit.

As with all of the means of evaluation, this is an imperfect measurement. Given their technical interests and the fast-paced nature of that field, many of the technologies that these users are interested in today may not have existed at the time the data was collected. Still, it does allow a comparison to show that the recommendations provided have value above random recommendation and because it is automated it allows far more documents to be ranked than the user study.



\subsection{Comparison to Current Network State}
\label{sec:ComparisonToCurrentNetwork}

Much of the research on link prediction in social networks focuses on predicting whether links will be created between users in the future, and the evaluation involves comparing the predicted links to those actually formed later. For this project, the only data available on the state of the social graph is that of the dataset and that of the present day.

Thus, one evaluation method used was to compare the users recommended for each distinguished user to the set of actual followees of that person in the present social network. For the two known users it was also possible to include users who were included in the recommendations whom the distinguished user may once have followed but no longer does. The number of recommended users likely to be in the set of present-day followees is extremely small given that most users follow a small set of people, but by comparing the number of users in the top 25 recommended users who overlap with the present-day followee list to the probability of a random user appearing in that list it is possible to demonstrate that the recommendations have value.

Calculating the probability of a random user from the data being amongst the present-day followees requires a major assumption: that the network today is the same size and has the same users as the network of the dataset. This is necessary because there simply is not data available for the relevant network information at any point except the dataset used here and the present day. This assumption is obviously not true, but it means that the actual probability is smaller than the calculated probability, so the calculated probability can be taken as an upper boundary. Using this assumption, it is possible to calculate the approximate probability that a random user would be a present-day followee of the distinguished user with the following formula:


\begin{center}
\[
\frac{\Delta_{followees} }{count(user\ vertices) - count(dataset\ followees)}
\]
\end{center}


User $K_{i}$ has 43 followees in the dataset used here compared to 443 today and 72 followers in the dataset used here compared to 791 today. User $K_{j}$ has 105 followees in the dataset compared to 505 today and 87 followers in the dataset used here compared to 495 today. User $U$ has 279 followees in the dataset compared to 442 today to go along with 3,720 followers in the dataset compared to 7,191 today. Given these numbers, the probability that a random user would be a present day followee of each user is listed in Table~\ref{tab:RandomUserFolloweeProb}. These probabilities can then be compared to the number of users recommended by the algorithm who the user currently follows in order to establish the value of the algorithm.

\begin{table}
\centering
\begin{tabular}{c|c|l}
{\bf User} & {\bf $\Delta_{followees}$ } & {\bf Probability} \\ \hline
$U$ & 163 &  0.04\%   \\ \hline
$K_{i}$ & 400 & 0.10\% \\ \hline
$K_{j}$ & 400 & 0.09\% \\
\end{tabular}
\caption{Probability of a random user being added as a followee}
\label{tab:RandomUserFolloweeProb}
\end{table}


\section{Results}

This section describes the results which were obtained. Most of the scores are presented in the form of precision at rank and are based on the results of the user study. These scores are labelled in the various tables as `P@5', `P@10', `P@15', and `P@20', representing the precision at 5, 10, 15, and 20. Recall from above that for purposes of calculating precision, a tweet or a user is considered to be relevant if it ranked at either 4 or 5 in the user study. Since user $U$ was not part of the user study, he has no numbers for this measurement.

For the user recommendation results, the tables also include a column for `\% 5s @ 5', `\% 5s @ 10', etc. This value is the percentage of users rated as a 5 at various ranks in the recommendations. This corresponds to the users that they actually have started following since this data was collected in 2009, though in the case of the users included in the user study, at least one or two of the users ranked as 5 were users who they did not follow before seeing the recommendation. This measurement is the metric described in Section~\ref{sec:ComparisonToCurrentNetwork} of comparing the list of recommended users to the users that the distinguished user now follows. Because it is possible to determine this for all users, this number is included for user $U$. These numbers compare very favourably with the chance of a random user appearing in this list as discussed in Section~\ref{sec:ComparisonToCurrentNetwork}, demonstrating that the user recommendations are very useful, though as will become apparent most of the value in user recommendations comes from the initial score.


\subsection{Baseline Results}
\label{sec:BaselineResults}

The results presented here as the baseline results are based on a $\lambda_{users}$ parameter value of 0.7 and a $\lambda_{tweets}$ parameter value of 0.9. These values represent the fact that the initial scores for the users are more useful as a basis for recommendation than the initial scores of the tweets.

It is clear just from looking at the initial scores for both users and tweets that the tweet scores are significantly less valuable than the user scores. While the initial user scores would provide an excellent ranking before the algorithm is even run, the initial tweet scores are not nearly so useful. By keeping the value of $\lambda_{tweets}$ closer to 1 the impact of these less useful initial scores on the final outcome is mitigated.

These results are also based on the presence of all of the edge types listed in Table~\ref{tab:EdgeTypes} in Section~\ref{sec:EdgeCreation}, with the directionality indicated there and an equal weighting for all edges.


\subsubsection{User Recommendation Results}

For all users, the run under these default parameters provided very good results on the user recommendations. In particular, user $K_{i}$ remarked that the user rankings were very good and that some of the recommended users for him were people that he found interesting enough to start following after seeing the recommendation. Table~\ref{tab:BaseUserResults} shows the results for user recommendation using the base configuration and compares these to the results obtained from just the initial scores. Note that in the case of the user $U$, the scores were the same for both.


\begin{table}
\centering
\begin{tabular}{l|c|p{2.25cm}|c|p{2.25cm}|c}
{\bf Metric} & {\bf User $U$} & {\bf User $K_{i}$ Init. Score} & {\bf User $K_{i}$} & {\bf User $K_{j}$ Init. Score} & {\bf User $K_{j}$} \\ \hline
P@5   & - & 0.6 & 0.6 & 0.8 & 0.6 \\ \hline
P@10 & - & 0.8 & 0.8 & 0.7 & 0.6 \\ \hline
P@15 & - & 0.533 & 0.733 & 0.666 & 0.6 \\ \hline
P@20 & - & 0.6 & 0.7 & 0.6 & 0.55 \\ \hline

\% 5s@5    & 0\% & 40\% & 20\% & 40\% & 40\% \\ \hline
\% 5s@10  & 0\% & 30\% & 30\% & 40\% & 40\% \\ \hline
\% 5s@15  & 0\% & 20\% & 26.6\% & 33\% & 33\% \\ \hline
\% 5s@20  & 0\% & 25\% & 25\% & 25\% & 25\% \\ \hline

Avg. Score @5   & - & 3.2 & 3.0 & 3.6 & 3.4 \\ \hline
Avg. Score @10 & - & 3.7 & 3.7 & 3.8 & 3.5 \\ \hline
Avg. Score @15 & - & 3.4 & 3.733 & 3.666 & 3.6 \\ \hline
Avg. Score @20 & - & 3.55 & 3.65 & 3.6 & 3.35 \\
\end{tabular}
\caption{The user recommendation results achieved using only the initial user scores and using the algorithm in the baseline configuration}
\label{tab:BaseUserResults}
\end{table}



These results are quite good, but at least for the case of these user recommendations the algorithm provides little to no improvement over the results that would be achieved if using only the initial score. As will become clear, however, the major value of these accurate user scores are that they make it possible to drastically improve the quality of the tweet recommendations over the initial scores.



\subsubsection{Tweet Recommendation Results}

The method of using cosine similarity for comparison as described in Section~\ref{sec:ComparisonToCurrentTweets} turned out not to be particularly useful, though it did demonstrate at a rudimentary level that the recommended tweets were at least somewhat valuable over noise. Because the information provided by this metric was of limited utility, it was only done for one set of recommendations. The results below come from the recommendations provided by the optimal configuration described in Section~\ref{sec:OptimalConfiguration} rather than for the base configuration, but since the tweets themselves recommended by these two configurations were so similar the results of this test would be similar as well.

Table~\ref{tab:RecommendedTweetSimilarity} shows the average cosine similarity between the recommended tweets and the reference tweets for user $U$. The table shows, for example, that the average of the similarity of the first 20 recommended tweets was 0.00330 for the reference set consisting of the tweets authored in the present day by the distinguished user and 0.00268 for the reference set consisting of the tweets retweeted in the present day by the distinguished user.


\begin{table}
\centering
\begin{tabular}{p{3.5cm}|p{4cm}|p{4cm}}
{\bf Recommendation List Position} & {\bf Authored Tweets Similarity} & {\bf Retweeted Tweets Similarity} \\ \hline
 5 & 0.00286 & 0.00100 \\ \hline
10 & 0.00285 & 0.00190 \\ \hline
15 & 0.00308 & 0.00227 \\ \hline
20 & 0.00330 & 0.00268 \\
\end{tabular}
\caption{Average cosine similarity between recommended tweets and current tweets of user $U$}
\label{tab:RecommendedTweetSimilarity}
\end{table}

\begin{table}
\centering
\begin{tabular}{p{3.25cm}|p{4cm}|p{4cm}}
{\bf \# of Random Tweets} & {\bf Authored Tweets Similarity} & {\bf Retweeted Tweets Similarity} \\ \hline
 500 & 0.00217 & 0.00198 \\ \hline
1000 & 0.00215  & 0.00200 \\ \hline
1500 & 0.00214 & 0.00200 \\ \hline
2000 & 0.00211 & 0.00197 \\ \hline
2500 & 0.00210 & 0.00199 \\
\end{tabular}
\caption{Average cosine similarity between random tweets and current tweets of user $U$}
\label{tab:RandomTweetSimilarity}
\end{table}


Table~\ref{tab:RandomTweetSimilarity} shows the average cosine similarity between random tweets taken from the database and the various reference sets. The table shows, for example, that after all 2,500 tweets had been considered their average similarity to the reference set was 0.00210 and 0.00199 for the authored tweets and the retweeted tweets, respectively. The results from the recommended list of tweets are noticeably better than those for the random tweets. Still, given the inaccuracy of the cosine similarity measurement to begin with and the lack of a major difference in the similarity to retweeted tweets, this measurement was not particularly useful and will not be revisited. It does, however, suggest that the recommendations have at least some value.

The comparison between the current network and the results of the user study provided a much better source of results. These results are shown in Table~\ref{tab:BaseTweetResults}, including the comparison with the results obtained using the initial tweet scores only. Since no user study information was available for user $U$, he is not included in this table. After rating the recommended tweets as part of the user study, user $K_{i}$ mentioned that the recommendations were at least as good as those provided by Twitter. User $K_{j}$ was generally a harsher critic of the tweets and he remarked that in general he was quite discerning about what he liked on Twitter and would automatically dislike a tweet which contained spelling errors, which on Twitter provides a major limitation.


\begin{table}
\centering
\begin{tabular}{l|p{2.25cm}|c|p{2.25cm}|c}
{\bf Metric}& {\bf User $K_{i}$ Init. Score} & {\bf User $K_{i}$} & {\bf User $K_{j}$ Init. Score} & {\bf User $K_{j}$} \\ \hline
P@5   & 0 & 0.6 & 0.4 & 0.0 \\ \hline
P@10 & 0.1 & 0.6 & 0.3 & 0.2 \\ \hline
P@15 & 0.066 & 0.4 & 0.266 & 0.4 \\ \hline
P@20 & 0.05 & 0.3 & 0.2 & 0.4 \\ \hline
Avg. Score @5   & 1.8 & 3.4 & 2.6 & 2.8 \\ \hline
Avg. Score @10 & 2.4 & 3.3 & 2.9 & 3.1 \\ \hline
Avg. Score @15 & 2.133 & 2.933 & 2.666 & 3.266 \\ \hline
Avg. Score @20 & 2.05 & 2.9 & 2.55 & 3.3 \\
\end{tabular}
\caption{The tweet recommendation results achieved using only the initial tweet scores and using the algorithm in the baseline configuration}
\label{tab:BaseTweetResults}
\end{table}

As with the user recommendations, these results are very good. In the original Co-HITS paper (\cite{Deng2009}), Deng et. al. had results for precision at rank 5 of between ~0.35 and ~0.39 and precision at 10 of between ~0.31 and ~0.35 for a web search application, and the results here are in the same neighbourhood when averaged together. It is not surprising to see that the results from the base configuration are quite a bit better than those based off of the original tweet scores given how poor the tweets with the highest initial scores were.



\subsection{Varying $\lambda$ Parameters}
\label{sec:VaryingLambda}

The choice of the $\lambda$ parameters from the previous section was not arbitrary, but rather was based on experimentation to see which values for these parameters produced the best results. Two experiments were run. The first, on user $K_{i}$, held the value of $\lambda_{tweets}$ constant at 0.9 while varying the value of the $\lambda_{users}$ parameter. The second experiment was run on user $K_{j}$ and held the value of $\lambda_{users}$ constant at 0.7 while varying the value of the $\lambda_{tweets}$.

Recall that the closer the $\lambda$ parameters are to 0, the more of an impact the initial score has on the final outcome---the fact which drove the selection of $\lambda$ values to hold constant for each experiment. The results of these experiments for P@10 and P@20 are shown in .... [fill this in]

%TODO: say where! Need to get the data first, though.


\subsection{Varying Edge Types Included}
\label{sec:VaryingEdgeTypes}

As was mentioned in Chapter 4, the algorithm was slow to run due to database access times. This made it difficult to experiment with removing edge types because with 12 different edge types there were a large number of possible combinations that could have been removed. Instead, the experiments focused on removing the edge types that seemed like they might be less valuable.

Recall the edge types from Table~\ref{tab:EdgeTypes}. The retweet, mention, and at-reply edges connect the user who was retweeted, mentioned, or at-replied to with the tweet that retweeted, mentioned, or at-replied to them. The logic behind this was previously discussed, but the link is a tenuous one and thus these are prime candidates for removal to see the effect on the results. It is also far from clear which direction is the appropriate one, which is discussed in the section on varying edge directionality, Section~\ref{sec:VaryingEdgeDirectionality}.

There were 292,475 edges of these types, which is a very small amount in the context of 120 million total edges, but as can be seen from the results from removing these edges shown in Table~\ref{tab:Remove369}, removing these edges had a positive impact on the overall quality of the results when compared with the baseline results already presented. This experiment was done for user $K_{i}$ only %TODO is that true??

%{tab:Remove369}



The results from removing one or both of the types of content edges were far less clear. Three separate experiments were run: one that removed hashtag edges, one that removed the entity-based edges, and one that removed both. There were a lot of the content edges: about 42 million hashtag edges and about 38 million entity-based edges. It was surprising, then, when removing them had such a small effect. Comparing the results in Table~\ref{tab:RemoveContent} with the baseline results already presented shows very little difference. As with the other experiment with removing edges, this experiment was only run for user $K_{i}$.


%tab:RemoveContent


\subsection{Varying Edge Weights}

The slowness of running the algorithm made experimentation on edge weights particularly difficult because with so many different edges and possible weightings for them a rigorous experiment would require far more time than was available. As such, the experiments which were done were based largely on intuition and were small in number, with only four different weightings being explored.

The experiments done here were performed on user $K_{j}$, with directionality per the default, though authorship edges were bi-directional. Table~\ref{tab:EdgeWeightsUsed} shows the weights that were used for each edge type in each experiment. Table~\ref{tab:EdgeWeightResults} shows the results for each experiment for both user recommendations and tweet recommendations. Note that because of the results on varying edge types described in Section~\ref{sec:VaryingEdgeTypes}, the edges corresponding to retweets, mentions, and at replies were not included for any of these experiments, and thus no weights are listed.

The methodology used was to sum the total weight of all edges emanating from a vertex, and the chance of taking a particular edge was just the weight of that edge's type divided by the total weight of all edges. So if a particular vertex had two edges with a weight of 2 each and one edge with a weight of  0.5 then the total weight would be 4.5, so the chance of going to each of the edges with weight 2 would be $2 \div 4.5 = 0.44$ and the chance of going to the edge with weight 0.5 would be $0.5 \div 4.5 = 0.11$.


\begin{table}
\centering
\begin{tabular}{l|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}}
{\bf Edge Type} & {\bf Test 1 Weight} & {\bf Test 2 Weight} & {\bf Test 3 Weight} & {\bf Test 4 Weight} \\ \hline
Authorship & 2 & 2 & 2 & 4 \\ \hline
Follower & 1.25 & 1.25 & 1.25 & 1.5 \\ \hline
Retweet Followees & 2 & 3 & 3 & 3 \\ \hline
Retweet Followers & 0.75 & 1.25 & 1.25 & 1.25 \\ \hline
Mention Followees & 2 & 2 & 2 & 1.5 \\ \hline
Mention Followers & 0.75 & 1 & 1 & 1 \\ \hline
@reply Content & 0.5 & 0.5 & 0.25 & 0.5 \\ \hline
Hashtag & 1 & 1 & 3.5 & 1 \\ \hline
Content & 1 & 1 & 1.5 & 1 \\
\end{tabular}
\caption{Edge weights used for each experiment }
\label{tab:EdgeWeightsUsed}
\end{table}

%TODO: need to insert the results here -- should there be a discussion about weighting 3,6,9?



\subsection{Varying Edge Directionality}
\label{sec:VaryingEdgeDirectionality}

The directionality of the edges when producing the baseline results described in Section~\ref{sec:BaselineResults} followed the description in Table~\ref{tab:EdgeTypes}. As shown in that section, this directionality produced generally good recommendations for both users and tweets.

One obvious variation on these baseline results is to remove edge directionality altogether and assume that each edge type has an influence in both directions. For at least some of the edge types, this makes perfect sense. Consider the authorship edge, for example. It certainly makes sense that if a particular tweet received a high score because of something else that it was connected to then that score should be passed along to the author of the tweet. Similarly, a highly scoring user should obviously transfer some of that high score along to the tweets that they author.

Running the algorithm with all edge types being bi-directional was one of the first experiments that was run for user $U$. The results from doing this were very clearly terrible. First, the tweets were dominated by one or two users, and the tweets of those users were political in nature and also diametrically opposed to the political views of the distinguished user. On the scale used for the user study, all of these would have ranked as a 2.

The reason for this was that the user who produced the tweets followed over 40,000 people, and today follows more than 80,000 people. Clearly this user does not interact with or even read all of the tweets from these 40,000 people, but the algorithm cannot take that into account. So because this user tweeted frequently and followed so many people, their tweets were highly scored by the system with bi-directional edges because each of their 19 tweets was connected to thousands of users whose scores all contributed to high scores for these tweets.

This result demonstrates that for the edges based on the network state it does not makes sense to vary the directionality of the edges, either by reversing them or by making them bi-directional. Take follower edges for example: clearly there is not an influence by a user on the tweets of someone they follow. This was the biggest issue in the experiment done with no directionality at all---it assumed influence where there was none and came up with correspondingly terrible results.

Eliminating the network edges from consideration then, leaves only the two content-based edges, the authorship edge, and the basic retweet, mention, and at-reply edges which were discussed in the context of removing them in Section~\ref{sec:VaryingEdgeTypes}. With these edges it is not clear that any one direction should be the one that has the influence; for instance, it could make sense that all tweets on a given subject should provide their scores to a user or that a highly scoring user should provide an impact on all tweets mentioning subjects of a similar subject. Similarly, leaving these as bi-directional also makes sense.

Most of the experiments on edge directionality were run on user $K_{i}$, though the user is indicated in each case.

%experiments include content directions, author directions, and 3,6,9 directions


\subsection{Optimal Configuration}
\label{sec:OptimalConfiguration}

After all of the experiments, it was clear which were the best values for the $\lambda$ parameters, which were the best edge directions, and which were the best edges to include. Combining all of these into one complete configuration produced the best results of all. This configuration had $\lambda_{users}$ at 0.7, $\lambda_{tweets}$ at 0.9, and the edge directionality as in Table~\ref{tab:EdgeTypes} except with the authorship edges being bi-directional. This configuration also omitted the retweet, mention, and at reply edges which connect the tweet to the person being retweeted, mentioned, or replied to. The results for this configuration are shown in Table~\ref{tab:OptimalConfigTweetResults} for tweets and in Table~\ref{tab:OptimalConfigUserResults} for users. These can be compared (very favourably) with the results based purely on the original scores, which are shown in Table~\ref{tab:OriginalScoreUserResults}.

%TODO: table for optimal config, similar to those for base results for users and tweets

\section{Dataset Biases}

The results described in this chapter revealed some issues that come with selecting tweets to use for such a study that can end up having a major impact on the final results.

One drawback of the Twitter network is that particular communities are often over-represented, which can make it more difficult to find relevant content for people not in those communities. The tweet recommendations here show that effect quite clearly. User $U$ is a part of the Silicon Valley entrepreneurship community, which is very strongly represented on Twitter. Within the 500,000 tweets studied here that user's results were very good since there were a large number of tweets that would be interesting to someone from that community. But the trade-off is that there were far fewer tweets of interest available for the other users who were not part of that community.

Another bias in the Twitter dataset is that the subjects of the tweets can be overwhelmed by one particular event. The day that the tweets used here were collected happened to be World AIDS Day in 2009, resulting in a much higher level of discussion of things related to that than would normally be expected. This also had the effect of limiting the number of interesting tweets for someone who wasn't interested in content related to World AIDS Day.

Finally, because of limitations within the Twitter API and limitations of storing and collecting a large amount of data, the Twitter data used here only represents 20-30\% of tweets during the time period. This results in a lot of cases where a retweet is among the recommendations while the original tweet was never seen. The results could be improved if the repeated retweets of the same tweet were removed from the recommendations in favour of the original tweet.

All of these biases were noticeable in the results, though the impact was generally not a major one and the algorithm still produced excellent recommendations. It would be interesting and valuable to study the effect that these biases have on the results, such as by studying a larger number of tweets, a more complete dataset, or multiple different time periods.

That is far from the only work that remains to be done in this still very new research area, of course. While the results here are very promising, the experiments and methodologies suggest that plenty of improvements could be made and further experiments performed to improve the results described in this chapter. The final chapter will discuss some of the other future work that could be done to expand on and improve this project as well as providing concluding remarks on the project as a whole.





